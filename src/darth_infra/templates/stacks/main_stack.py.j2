"""Main CDK stack — orchestrates all environment-specific resources.

Generated by darth-infra. You may customize this file; re-running ``darth-infra init``
will not overwrite existing files unless ``--force`` is passed.
"""

from __future__ import annotations

import json
import sys
from pathlib import Path

import boto3
from aws_cdk import RemovalPolicy, Stack, aws_ec2, aws_ecs, aws_ssm
{% if has_service_discovery %}
from aws_cdk import aws_servicediscovery
{% endif %}
from constructs import Construct

if sys.version_info >= (3, 11):
    import tomllib
else:
    import tomli as tomllib  # type: ignore[import-not-found]

from .constructs.ecr_repository import EcrRepository
from .constructs.ecs_service import EcsService
{% if has_rds %}
from .constructs.rds_database import RdsDatabase
{% endif %}
{% if has_s3 %}
from .constructs.s3_bucket import AppS3Bucket
{% endif %}
{% if has_cloudfront %}
from .constructs.cloudfront_distribution import AppCloudFront
{% endif %}
from .constructs.alb import AlbProvider
from .constructs.secrets import SecretsProvider


# ---------------------------------------------------------------------------
# Lightweight config types (mirrors darth-infra models, but dependency-free)
# ---------------------------------------------------------------------------

from dataclasses import dataclass, field
from typing import Any


@dataclass
class _ServiceCfg:
    name: str
    dockerfile: str = "Dockerfile"
    build_context: str = "."
    port: int | None = 8000
    health_check_path: str = "/health"
    cpu: int = 256
    memory_mib: int = 512
    desired_count: int = 1
    command: str | None = None
    domain: str | None = None
    secrets: list[str] = field(default_factory=list)
    s3_access: list[str] = field(default_factory=list)
    environment_variables: dict[str, str] = field(default_factory=dict)
    enable_exec: bool = True
    launch_type: str = "fargate"
    ec2_instance_type: str | None = None
    user_data_script: str | None = None
    ebs_volumes: list[dict[str, Any]] = field(default_factory=list)
    enable_service_discovery: bool = False


@dataclass
class _RdsCfg:
    database_name: str
    instance_type: str = "t4g.micro"
    allocated_storage_gb: int = 20
    expose_to: list[str] = field(default_factory=list)
    engine_version: str = "15"
    backup_retention_days: int = 7


@dataclass
class _S3Cfg:
    name: str
    public_read: bool = False
    cloudfront: bool = False
    cors: bool = False


@dataclass
class _SecretCfg:
    name: str
    source: str = "generate"
    length: int = 50
    generate_once: bool = True


@dataclass
class _AlbCfg:
    mode: str = "shared"
    shared_alb_name: str = ""
    certificate_arn: str | None = None


@dataclass
class _EnvOverride:
    domain_overrides: dict[str, str] = field(default_factory=dict)
    instance_type_override: str | None = None
    ec2_instance_type_override: dict[str, str] = field(default_factory=dict)


@dataclass
class ProjectConfig:
    project_name: str
    services: list[_ServiceCfg]
    environments: list[str] = field(default_factory=lambda: ["prod"])
    aws_region: str = "us-east-1"
    vpc_name: str = "artshumrc-prod-standard"
    rds: _RdsCfg | None = None
    s3_buckets: list[_S3Cfg] = field(default_factory=list)
    alb: _AlbCfg = field(default_factory=_AlbCfg)
    secrets: list[_SecretCfg] = field(default_factory=list)
    environment_overrides: dict[str, _EnvOverride] = field(default_factory=dict)
    tags: dict[str, str] = field(default_factory=dict)

    def domain_for(self, service_name: str, env: str) -> str | None:
        svc = next((s for s in self.services if s.name == service_name), None)
        if svc is None or svc.domain is None:
            return None
        ov = self.environment_overrides.get(env)
        if ov and service_name in ov.domain_overrides:
            return ov.domain_overrides[service_name]
        return svc.domain if env == "prod" else f"{env}-{svc.domain}"


def load_project_config() -> ProjectConfig:
    """Load ``darth-infra.toml`` from the project root."""
    cfg_path = Path(__file__).resolve().parent.parent / "darth-infra.toml"
    with open(cfg_path, "rb") as f:
        raw = tomllib.load(f)

    p = raw.get("project", {})
    return ProjectConfig(
        project_name=p["name"],
        aws_region=p.get("aws_region", "us-east-1"),
        vpc_name=p.get("vpc_name", "artshumrc-prod-standard"),
        environments=p.get("environments", ["prod"]),
        tags=p.get("tags", {}),
        services=[_ServiceCfg(**s) for s in raw.get("services", [])],
        rds=_RdsCfg(**raw["rds"]) if raw.get("rds") else None,
        s3_buckets=[_S3Cfg(**b) for b in raw.get("s3_buckets", [])],
        alb=_AlbCfg(**raw.get("alb", {})),
        secrets=[_SecretCfg(**s) for s in raw.get("secrets", [])],
        environment_overrides={
            k: _EnvOverride(**v)
            for k, v in raw.get("environments", {}).items()
            if isinstance(v, dict)
        },
    )


class MainStack(Stack):
    """Deploys all environments for **{{ project_name }}**."""

    def __init__(
        self,
        scope: Construct,
        construct_id: str,
        *,
        config: ProjectConfig,
        **kwargs: Any,
    ) -> None:
        super().__init__(scope, construct_id, **kwargs)

        self.config = config
        vpc = aws_ec2.Vpc.from_lookup(
            self, "Vpc", vpc_name=config.vpc_name
        )

        # ECR repositories — shared across environments (one per service)
        ecr_repos: dict[str, EcrRepository] = {}
        for svc in config.services:
            ecr_repos[svc.name] = EcrRepository(
                self,
                f"Ecr-{svc.name}",
                project_name=config.project_name,
                service_name=svc.name,
            )

        # Per-environment resources
        for env_name in config.environments:
            self._deploy_environment(vpc, config, env_name, ecr_repos)

    # ------------------------------------------------------------------

    def _deploy_environment(
        self,
        vpc: aws_ec2.IVpc,
        config: ProjectConfig,
        env_name: str,
        ecr_repos: dict[str, EcrRepository],
    ) -> None:
        prefix = f"{config.project_name}-{env_name}"

        # --- Secrets ---------------------------------------------------------
        secrets_provider = SecretsProvider(
            self,
            f"Secrets-{env_name}",
            project_name=config.project_name,
            env_name=env_name,
            secret_configs=config.secrets,
        )

        # --- ALB -------------------------------------------------------------
        alb_provider = AlbProvider(
            self,
            f"Alb-{env_name}",
            vpc=vpc,
            project_name=config.project_name,
            env_name=env_name,
            alb_mode=config.alb.mode,
            shared_alb_name=config.alb.shared_alb_name,
            certificate_arn=config.alb.certificate_arn,
        )

        # --- RDS (optional) --------------------------------------------------
        rds_db = None
        {% if has_rds %}
        if config.rds:
            snapshot_id = None
            if env_name != "prod":
                snapshot_id = self._find_latest_prod_snapshot(config)

            rds_instance_type = config.rds.instance_type
            ov = config.environment_overrides.get(env_name)
            if ov and ov.instance_type_override:
                rds_instance_type = ov.instance_type_override

            rds_db = RdsDatabase(
                self,
                f"Rds-{env_name}",
                vpc=vpc,
                project_name=config.project_name,
                env_name=env_name,
                database_name=config.rds.database_name,
                instance_type_str=rds_instance_type,
                allocated_storage_gb=config.rds.allocated_storage_gb,
                engine_version=config.rds.engine_version,
                backup_retention_days=config.rds.backup_retention_days,
                snapshot_identifier=snapshot_id,
            )
        {% endif %}

        # --- S3 buckets (optional) -------------------------------------------
        s3_buckets: dict[str, "AppS3Bucket"] = {}
        {% if has_s3 %}
        for bucket_cfg in config.s3_buckets:
            s3_buckets[bucket_cfg.name] = AppS3Bucket(
                self,
                f"S3-{env_name}-{bucket_cfg.name}",
                project_name=config.project_name,
                env_name=env_name,
                bucket_name_suffix=bucket_cfg.name,
                public_read=bucket_cfg.public_read,
                cors=bucket_cfg.cors,
            )
        {% endif %}

        # --- CloudFront for S3 (optional) ------------------------------------
        {% if has_cloudfront %}
        for bucket_cfg in config.s3_buckets:
            if bucket_cfg.cloudfront and bucket_cfg.name in s3_buckets:
                AppCloudFront(
                    self,
                    f"CF-{env_name}-{bucket_cfg.name}",
                    bucket=s3_buckets[bucket_cfg.name].bucket,
                    project_name=config.project_name,
                    env_name=env_name,
                    bucket_name_suffix=bucket_cfg.name,
                )
        {% endif %}

        # --- ECS services ----------------------------------------------------
        # Shared ECS cluster — exactly one per environment
        cluster = aws_ecs.Cluster(
            self,
            f"Cluster-{env_name}",
            vpc=vpc,
            cluster_name=prefix,
            container_insights_v2=aws_ecs.ContainerInsights.ENABLED,
        )

        {% if has_service_discovery %}
        # Cloud Map private DNS namespace (when any service opts in)
        _any_discovery = any(
            s.enable_service_discovery for s in config.services
        )
        if _any_discovery:
            cluster.add_default_cloud_map_namespace(
                name="local",
                type=aws_servicediscovery.NamespaceType.DNS_PRIVATE,
                vpc=vpc,
            )
        {% endif %}

        {% if has_ec2 %}
        # Find EBS snapshots from prod for non-prod environments
        ebs_snapshot_map: dict[str, dict[str, str]] = {}
        if env_name != "prod":
            for svc in config.services:
                if svc.launch_type == "ec2" and svc.ebs_volumes:
                    ebs_snapshot_map[svc.name] = self._find_prod_ebs_snapshots(
                        config, svc.name, svc.ebs_volumes
                    )
        {% endif %}

        ecs_constructs: dict[str, EcsService] = {}
        for svc in config.services:
            domain = config.domain_for(svc.name, env_name)

            db_env_vars: dict[str, str] = {}
            db_secret_arns: dict[str, str] = {}
            {% if has_rds %}
            if rds_db and svc.name in (config.rds.expose_to if config.rds else []):
                db_env_vars["DB_HOST"] = rds_db.endpoint
                db_env_vars["DB_PORT"] = rds_db.port
                db_env_vars["DB_NAME"] = config.rds.database_name if config.rds else ""
                db_secret_arns["DB_PASSWORD"] = rds_db.credentials_secret_arn
                db_env_vars["DB_USER"] = config.rds.database_name if config.rds else ""
            {% endif %}

            # S3 bucket name env vars
            s3_env_vars: dict[str, str] = {}
            s3_bucket_arns: list[str] = []
            {% if has_s3 %}
            for bucket_name in svc.s3_access:
                if bucket_name in s3_buckets:
                    env_key = f"S3_BUCKET_{bucket_name.upper().replace('-', '_')}"
                    s3_env_vars[env_key] = s3_buckets[bucket_name].bucket.bucket_name
                    s3_bucket_arns.append(s3_buckets[bucket_name].bucket.bucket_arn)
            {% endif %}

            ecs_svc = EcsService(
                self,
                f"Ecs-{env_name}-{svc.name}",
                vpc=vpc,
                cluster=cluster,
                project_name=config.project_name,
                env_name=env_name,
                service_name=svc.name,
                ecr_repo=ecr_repos[svc.name].repository,
                container_port=svc.port,
                health_check_path=svc.health_check_path,
                cpu=svc.cpu,
                memory_mib=svc.memory_mib,
                desired_count=svc.desired_count,
                command=svc.command,
                domain=domain,
                extra_env_vars={**svc.environment_variables, **db_env_vars, **s3_env_vars},
                secret_arns={**secrets_provider.get_arns_for(svc.secrets), **db_secret_arns},
                s3_bucket_arns=s3_bucket_arns,
                alb_provider=alb_provider,
                enable_exec=svc.enable_exec,
                launch_type=svc.launch_type,
                ec2_instance_type=self._resolve_ec2_instance_type(config, svc.name, env_name),
                user_data_script=svc.user_data_script,
                ebs_volumes=svc.ebs_volumes,
                {% if has_ec2 %}
                ebs_snapshot_ids=ebs_snapshot_map.get(svc.name, {}),
                {% endif %}
                {% if has_service_discovery %}
                cloud_map_service_name=svc.name if svc.enable_service_discovery else None,
                {% endif %}
            )

            ecs_constructs[svc.name] = ecs_svc

            # Wire security groups
            {% if has_rds %}
            if rds_db and svc.name in (config.rds.expose_to if config.rds else []):
                rds_db.allow_from(ecs_svc.task_security_group)
            {% endif %}

        {% if has_service_discovery %}
        # Inter-service security group wiring for discovery-enabled services
        _discovery_svcs = [
            (svc_name, svc_construct)
            for svc_name, svc_construct in ecs_constructs.items()
        ]
        _discovery_names = [
            s.name for s in config.services if s.enable_service_discovery
        ]
        for i, name_a in enumerate(_discovery_names):
            for name_b in _discovery_names[i + 1:]:
                if name_a in ecs_constructs and name_b in ecs_constructs:
                    ecs_constructs[name_a].task_security_group.add_ingress_rule(
                        ecs_constructs[name_b].task_security_group,
                        aws_ec2.Port.all_tcp(),
                        f"{name_b} -> {name_a} (service discovery)",
                    )
                    ecs_constructs[name_b].task_security_group.add_ingress_rule(
                        ecs_constructs[name_a].task_security_group,
                        aws_ec2.Port.all_tcp(),
                        f"{name_a} -> {name_b} (service discovery)",
                    )
        {% endif %}

    {% if has_rds %}

    def _find_latest_prod_snapshot(self, config: ProjectConfig) -> str | None:
        """Find the latest automated RDS snapshot from the prod instance."""
        try:
            rds_client = boto3.client("rds", region_name=config.aws_region)
            db_id = f"{config.project_name}-prod-db"
            response = rds_client.describe_db_snapshots(
                DBInstanceIdentifier=db_id,
                SnapshotType="automated",
            )
            snapshots = sorted(
                response.get("DBSnapshots", []),
                key=lambda s: s.get("SnapshotCreateTime", ""),
                reverse=True,
            )
            if snapshots:
                return snapshots[0]["DBSnapshotIdentifier"]
        except Exception:
            pass
        return None
    {% endif %}

    def _resolve_ec2_instance_type(
        self, config: ProjectConfig, service_name: str, env_name: str
    ) -> str | None:
        """Resolve EC2 instance type, applying per-environment overrides."""
        svc = next((s for s in config.services if s.name == service_name), None)
        if svc is None or svc.launch_type != "ec2":
            return None
        ov = config.environment_overrides.get(env_name)
        if ov and service_name in ov.ec2_instance_type_override:
            return ov.ec2_instance_type_override[service_name]
        return svc.ec2_instance_type

    {% if has_ec2 %}

    def _find_prod_ebs_snapshots(
        self,
        config: ProjectConfig,
        service_name: str,
        ebs_volumes: list[dict[str, Any]],
    ) -> dict[str, str]:
        """Find the latest EBS snapshots from prod for each volume.

        Returns a mapping of volume name -> snapshot ID.
        """
        result: dict[str, str] = {}
        try:
            ec2_client = boto3.client("ec2", region_name=config.aws_region)
            for vol in ebs_volumes:
                response = ec2_client.describe_snapshots(
                    Filters=[
                        {"Name": "tag:Project", "Values": [config.project_name]},
                        {"Name": "tag:Environment", "Values": ["prod"]},
                        {"Name": "tag:Service", "Values": [service_name]},
                        {"Name": "tag:VolumeName", "Values": [vol["name"]]},
                    ],
                    OwnerIds=["self"],
                )
                snapshots = sorted(
                    response.get("Snapshots", []),
                    key=lambda s: s.get("StartTime", ""),
                    reverse=True,
                )
                if snapshots:
                    result[vol["name"]] = snapshots[0]["SnapshotId"]
        except Exception:
            pass
        return result
    {% endif %}
