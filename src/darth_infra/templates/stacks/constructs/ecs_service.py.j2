"""ECS service construct â€” supports both Fargate and EC2 launch types."""

from __future__ import annotations

from pathlib import Path
from typing import Any

from aws_cdk import (
    Duration,
    RemovalPolicy,
    Stack,
    Tags,
    aws_autoscaling,
    aws_ec2,
    aws_ecr,
    aws_ecs,
    aws_iam,
    aws_logs,
    aws_secretsmanager,
)
from constructs import Construct

from .alb import AlbProvider


class EcsService(Construct):
    """Deploys a single ECS service (Fargate or EC2) behind an ALB."""

    def __init__(
        self,
        scope: Construct,
        construct_id: str,
        *,
        vpc: aws_ec2.IVpc,
        cluster: aws_ecs.Cluster,
        project_name: str,
        env_name: str,
        service_name: str,
        ecr_repo: aws_ecr.IRepository,
        container_port: int | None,
        health_check_path: str,
        cpu: int,
        memory_mib: int,
        desired_count: int,
        command: str | None,
        domain: str | None,
        extra_env_vars: dict[str, str],
        secret_arns: dict[str, str],
        s3_bucket_arns: list[str],
        alb_provider: AlbProvider,
        enable_exec: bool = True,
        launch_type: str = "fargate",
        ec2_instance_type: str | None = None,
        user_data_script: str | None = None,
        ebs_volumes: list[dict[str, Any]] | None = None,
        ebs_snapshot_ids: dict[str, str] | None = None,
        cloud_map_service_name: str | None = None,
    ) -> None:
        super().__init__(scope, construct_id)

        prefix = f"{project_name}-{env_name}-{service_name}"
        ebs_volumes = ebs_volumes or []
        ebs_snapshot_ids = ebs_snapshot_ids or {}

        # --- Log group -------------------------------------------------------
        log_group = aws_logs.LogGroup(
            self,
            "Logs",
            log_group_name=f"/ecs/{prefix}",
            retention=aws_logs.RetentionDays.ONE_MONTH,
            removal_policy=RemovalPolicy.DESTROY,
        )

        # --- Security group --------------------------------------------------
        self.task_security_group = aws_ec2.SecurityGroup(
            self,
            "TaskSg",
            vpc=vpc,
            description=f"{prefix} ECS tasks",
            allow_all_outbound=True,
        )

        # --- IAM roles -------------------------------------------------------
        execution_role = aws_iam.Role(
            self,
            "ExecRole",
            assumed_by=aws_iam.ServicePrincipal("ecs-tasks.amazonaws.com"),
            managed_policies=[
                aws_iam.ManagedPolicy.from_aws_managed_policy_name(
                    "service-role/AmazonECSTaskExecutionRolePolicy"
                ),
            ],
        )

        task_role = aws_iam.Role(
            self,
            "TaskRole",
            assumed_by=aws_iam.ServicePrincipal("ecs-tasks.amazonaws.com"),
        )

        # ECS Exec permissions
        if enable_exec:
            task_role.add_to_policy(
                aws_iam.PolicyStatement(
                    actions=[
                        "ssmmessages:CreateControlChannel",
                        "ssmmessages:CreateDataChannel",
                        "ssmmessages:OpenControlChannel",
                        "ssmmessages:OpenDataChannel",
                    ],
                    resources=["*"],
                )
            )

        # S3 permissions
        if s3_bucket_arns:
            task_role.add_to_policy(
                aws_iam.PolicyStatement(
                    actions=[
                        "s3:GetObject",
                        "s3:PutObject",
                        "s3:DeleteObject",
                        "s3:ListBucket",
                    ],
                    resources=[
                        arn for bucket_arn in s3_bucket_arns
                        for arn in [bucket_arn, f"{bucket_arn}/*"]
                    ],
                )
            )

        # Secrets Manager read permissions
        if secret_arns:
            for secret_name, arn in secret_arns.items():
                aws_secretsmanager.Secret.from_secret_complete_arn(
                    self, f"SecretRef-{secret_name}", secret_complete_arn=arn
                ).grant_read(execution_role)

        # --- EC2 capacity provider (when launch_type is "ec2") ---------------
        if launch_type == "ec2":
            # Build user data: always register with ECS cluster, then append
            # the user's custom script if provided.
            user_data = aws_ec2.UserData.for_linux()
            user_data.add_commands(
                f'echo "ECS_CLUSTER={project_name}-{env_name}" >> /etc/ecs/ecs.config',
                'echo "ECS_ENABLE_TASK_IAM_ROLE=true" >> /etc/ecs/ecs.config',
            )

            if user_data_script:
                script_path = (
                    Path(__file__).resolve().parent.parent.parent / user_data_script
                )
                if script_path.is_file():
                    user_data.add_commands(script_path.read_text())

            # EBS block device mappings for the launch template
            block_devices: list[aws_autoscaling.BlockDevice] = []
            for vol in ebs_volumes:
                snap_id = ebs_snapshot_ids.get(vol["name"])
                ebs_props: dict[str, Any] = {
                    "volume_size": vol["size_gb"],
                    "volume_type": aws_autoscaling.EbsDeviceVolumeType.GP3
                    if vol.get("volume_type", "gp3") == "gp3"
                    else aws_autoscaling.EbsDeviceVolumeType.GP2,
                    "delete_on_termination": False,
                }
                if snap_id:
                    ebs_props["snapshot_id"] = snap_id
                block_devices.append(
                    aws_autoscaling.BlockDevice(
                        device_name=vol.get("device_name", "/dev/xvdf"),
                        volume=aws_autoscaling.BlockDeviceVolume.ebs(**ebs_props),
                    )
                )

                # Mount the EBS device inside the user data
                user_data.add_commands(
                    f'# Mount EBS volume: {vol["name"]}',
                    f'mkdir -p {vol["mount_path"]}',
                    f'if ! blkid {vol.get("device_name", "/dev/xvdf")}; then',
                    f'  mkfs.ext4 {vol.get("device_name", "/dev/xvdf")}',
                    "fi",
                    f'{vol.get("device_name", "/dev/xvdf")} {vol["mount_path"]} ext4 defaults,nofail 0 2 | tee -a /etc/fstab',
                    f'mount {vol["mount_path"]}',
                )

            asg = aws_autoscaling.AutoScalingGroup(
                self,
                "Asg",
                vpc=vpc,
                instance_type=aws_ec2.InstanceType(ec2_instance_type or "t3.medium"),
                machine_image=aws_ecs.EcsOptimizedImage.amazon_linux2(),
                desired_capacity=desired_count,
                min_capacity=desired_count,
                max_capacity=max(desired_count * 2, 2),
                security_group=self.task_security_group,
                user_data=user_data,
                block_devices=block_devices if block_devices else None,
                vpc_subnets=aws_ec2.SubnetSelection(
                    subnet_type=aws_ec2.SubnetType.PRIVATE_WITH_EGRESS
                ),
            )

            # Tag the ASG instances for EBS snapshot discovery
            Tags.of(asg).add("Project", project_name)
            Tags.of(asg).add("Environment", env_name)
            Tags.of(asg).add("Service", service_name)
            for vol in ebs_volumes:
                Tags.of(asg).add(f"EbsVolume:{vol['name']}", vol.get("device_name", "/dev/xvdf"))

            capacity_provider = aws_ecs.AsgCapacityProvider(
                self,
                "AsgCp",
                auto_scaling_group=asg,
                capacity_provider_name=f"{prefix}-cp",
            )
            cluster.add_asg_capacity_provider(capacity_provider)

        # --- Task definition -------------------------------------------------
        if launch_type == "ec2":
            task_def = aws_ecs.Ec2TaskDefinition(
                self,
                "Task",
                execution_role=execution_role,
                task_role=task_role,
                family=prefix,
                network_mode=aws_ecs.NetworkMode.AWS_VPC,
            )

            # Add EBS host volumes to the task definition
            for vol in ebs_volumes:
                task_def.add_volume(
                    name=vol["name"],
                    host=aws_ecs.Host(source_path=vol["mount_path"]),
                )
        else:
            task_def = aws_ecs.FargateTaskDefinition(
                self,
                "Task",
                cpu=cpu,
                memory_limit_mib=memory_mib,
                execution_role=execution_role,
                task_role=task_role,
                family=prefix,
            )

        # Environment variables
        env_vars: dict[str, str] = {
            "ENVIRONMENT": env_name,
            "SERVICE_NAME": service_name,
            **extra_env_vars,
        }

        # Secrets from Secrets Manager
        ecs_secrets: dict[str, aws_ecs.Secret] = {}
        for secret_name, arn in secret_arns.items():
            ecs_secrets[secret_name] = aws_ecs.Secret.from_secrets_manager(
                aws_secretsmanager.Secret.from_secret_complete_arn(
                    self, f"EcsSecret-{secret_name}", secret_complete_arn=arn
                )
            )

        container_kwargs: dict[str, Any] = {
            "image": aws_ecs.ContainerImage.from_ecr_repository(
                ecr_repo, tag=f"{env_name}-latest"
            ),
            "logging": aws_ecs.LogDrivers.aws_logs(
                stream_prefix=service_name,
                log_group=log_group,
            ),
            "environment": env_vars,
            "secrets": ecs_secrets,
            "container_name": service_name,
        }

        # EC2 tasks need explicit cpu/memory on the container
        if launch_type == "ec2":
            container_kwargs["cpu"] = cpu
            container_kwargs["memory_limit_mib"] = memory_mib

        container = task_def.add_container("Container", **container_kwargs)

        if command:
            container.add_container_dependencies()  # no-op, just ensure order
            # Override the entrypoint with a shell command
            task_def_cfn = task_def.node.default_child
            # Use command override via container definition
            container_node = container.node.default_child
            container.add_environment("_CMD_OVERRIDE", command)

        if container_port is not None:
            container.add_port_mappings(
                aws_ecs.PortMapping(
                    container_port=container_port,
                    protocol=aws_ecs.Protocol.TCP,
                )
            )

        # Mount EBS volumes on the container (EC2 only)
        if launch_type == "ec2":
            for vol in ebs_volumes:
                container.add_mount_points(
                    aws_ecs.MountPoint(
                        container_path=vol["mount_path"],
                        source_volume=vol["name"],
                        read_only=False,
                    )
                )

        # --- ECS service -----------------------------------------------------
        cloud_map_opts = None
        if cloud_map_service_name:
            cloud_map_opts = aws_ecs.CloudMapOptions(
                name=cloud_map_service_name,
            )

        if launch_type == "ec2":
            service = aws_ecs.Ec2Service(
                self,
                "Service",
                cluster=cluster,
                task_definition=task_def,
                desired_count=desired_count,
                security_groups=[self.task_security_group],
                service_name=prefix,
                enable_execute_command=enable_exec,
                cloud_map_options=cloud_map_opts,
                vpc_subnets=aws_ec2.SubnetSelection(
                    subnet_type=aws_ec2.SubnetType.PRIVATE_WITH_EGRESS
                ),
            )
        else:
            service = aws_ecs.FargateService(
                self,
                "Service",
                cluster=cluster,
                task_definition=task_def,
                desired_count=desired_count,
                security_groups=[self.task_security_group],
                assign_public_ip=False,
                service_name=prefix,
                enable_execute_command=enable_exec,
                cloud_map_options=cloud_map_opts,
                vpc_subnets=aws_ec2.SubnetSelection(
                    subnet_type=aws_ec2.SubnetType.PRIVATE_WITH_EGRESS
                ),
            )

        # --- ALB integration -------------------------------------------------
        if container_port is not None and domain:
            alb_provider.task_security_group = self.task_security_group
            self.task_security_group.add_ingress_rule(
                alb_provider.alb_security_group,
                aws_ec2.Port.tcp(container_port),
                "ALB to ECS task",
            )
            alb_provider.add_target(
                f"{env_name}-{service_name}",
                targets=[service],
                port=container_port,
                health_check_path=health_check_path,
                host_header=domain,
            )
